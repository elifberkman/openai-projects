{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OPENAI INTRODUCTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Environment Variable for OpenAI Api Key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Read Api key from text file\n",
    "with open('openai-api-key.txt', 'r') as f:\n",
    "    openai_api_key = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create Environment Variable\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Give me two reasons to learn OpenAI API with Python',\n",
    "    max_tokens=300\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. OpenAI API is the newest and most advanced library for Artificial Intelligence, Deep Learning and Machine Learning. This makes it the perfect tool for learning and mastering the most cutting-edge techniques in the industry. By leveraging OpenAI APIs with Python, one can easily apply the latest breakthroughs and build more sophisticated AI solutions.\n",
      "\n",
      "2. OpenAI is well supported within the Python programming language and offers a vast array of powerful frameworks that enable developers to quickly and easily create powerful AI solutions. This means that developers can get started quickly with OpenAI, as it is well-documented and provide an easy to use interface. Additionally, OpenAI libraries are constantly updated, meaning solutions created with OpenAI stay up to date with the latest in AI technology.\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HISTORY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2015 - OpenAI as a Non-Profit**\n",
    "\n",
    "- In December 2015, at the end of the NIPS (Neural Information Processing Systems) conference in Montreal, Elon Musk and Sam Altman announced the creation of a new non-profit organization, called OpenAI\n",
    "\n",
    "**Sam Altman**\n",
    "\n",
    "- Dropped out of Stanford to create a location-based social network called Loopt in 2005, which was later acquired in 2012 for &#36;43.4 million.\n",
    "- In 2011 Altman started as a part-time partner at startup accelerator Y Combinator. In 2014, he became the president of YC.\n",
    "- Y Combinator super famous accelerator in Silicon Valley. Lots of famous companies have come from this, like Airbnb and Dropbox.\n",
    "- Even though OpenAI started in 2015, it wasn’t until 2019 that Altman transitioned away from being president of YC to focus on OpenAI.\n",
    "- Gary Tan is now currently the president of YC.\n",
    "\n",
    "**Elon Musk**\n",
    "\n",
    "<i>1995</i>\n",
    "- Starts Zip2 with his brother Kimbal Musk and Greg Kouri.\n",
    "- Sold to Compaq in 1999 for &#36; 307 million (Elon’s stake was worth about &#36; 7 million).\n",
    "\n",
    "<i>1999</i>\n",
    "- Starts online financial services company X.com, which in 2000 merged with Confinity to form PayPal.\n",
    "\n",
    "<i>2002</i>\n",
    "- Paypal is acquired by eBay for &#36;1.5 billion.\n",
    "- Musk’s stake was approximately &#36;176 million.\n",
    "- Musk founded the company SpaceX, a rocket manufacturer and launcher.\n",
    "- Later on SpaceX would develop its own satellite internet.\n",
    "\n",
    "<i>2004</i>\n",
    "- Musk invests in Tesla, an electric car company, becoming its largest shareholder.\n",
    "- Starting in 2005 Musk took a much more active role in Tesla.\n",
    "\n",
    "<i>2022</i>\n",
    "- Musk acquires Twitter for &#36;43 billion.\n",
    "\n",
    "<i>Other companies:</i>\n",
    "- Solar City (acquired by Tesla)\n",
    "- Boring Company\n",
    "- Neuralink\n",
    "- Starlink\n",
    "- OpenAI\n",
    "\n",
    "**Early Major Investors in OpenAI**\n",
    "\n",
    "- Reid Hoffman (founder of LinkedIn)\n",
    "- Jessica Livingston (one of the founders of YC)\n",
    "- Peter Thiel\n",
    "- Infosys\n",
    "- Khosla Ventures\n",
    "- YC Research\n",
    "\n",
    "**Early Employees at OpenAI**\n",
    "\n",
    "- Greg Brockman\n",
    "- Ilya Sutskever\n",
    "- Trevor Blackwell\n",
    "- Andrej Karpathy\n",
    "- Durk Kingma\n",
    "- Wojciech Zaremba\n",
    "- And many more well-regarded advisors!\n",
    "\n",
    "**OpenAI**\n",
    "\n",
    "- Initially formed as a non-profit to safely develop Artificial Intelligence.\n",
    "- Musk and Altman were influenced by Google acquiring DeepMind in 2014, concerned that A.I. technology would only be developed and controlled by just a few of the world’s largest technology companies.\n",
    "- In 2016, OpenAI released the Gym library, which allowed for an easy to use environment for reinforcement learning.\n",
    "- In 2018, OpenAI announced the first version of GPT (Generative Pre-Training Transformer).\n",
    "- In 2018 Elon Musk resigned his board seat at OpenAI, due to “potential future conflict of interest” due to Tesla’s own development of AI systems (mainly for self-driving cars at the time, but now Tesla is working on a humanoid robot called Optimus).\n",
    "- In 2019, OpenAI transitioned from a non-profit organization to a “capped” for-profit organization, in order to accept an investment of &#36; 1 billion dollars, partnering with Microsoft in the process (who was also the lead investor).\n",
    "- In 2019, OpenAI announces a new model called GPT-2.\n",
    "- GPT-2 was not initially released to the public due to safety concerns regarding the ability to possibly create false misinformation at a large scale.\n",
    "- In 2020, GPT-3 was announced.\n",
    "- In June of 2020 OpenAI would announced the creation of an API to access its new AI Models.\n",
    "- Initial users had to apply to be accepted to have access to the API.\n",
    "- In 2021, OpenAI announced the creation of DALL-E, a model capable of producing images from text.\n",
    "- The model is not open-sourced or available via an API.\n",
    "- In 2022, DALLE-2 is announced, creating much higher fidelity images from text prompts.\n",
    "- Also in 2022 ChatGPT is announced, which is an optimized version of GPT for dialogue, trained on human feedback.\n",
    "- At the start of 2023, OpenAI announced that Microsoft made a new &#36; 10 billion investment for OpenAI.\n",
    "- As part of this investment, Azure became the exclusive cloud provider of OpenAI model API calls. Note that the API is still available directly from OpenAI.\n",
    "- Clearly a lot has happened in just a short timespan, and the pace of development is exponential!\n",
    "- If you’re interested in learning more about this recent history of artificial intelligence and the role its played in the dynamics of technology companies, you may want to read Genius Makers by Metz."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOW IT WORKS?\n",
    "\n",
    "**GPT**\n",
    "\n",
    "- One of the more novel aspects of GPT-3 versus its predecessors GPT and GPT-2 was its size.\n",
    "- GPT-3 has 175 billion parameters, which in storage terms is approximately 800 GB.\n",
    "- It also cost about &#36;4.6 million dollars to train in GPU costs.\n",
    "- GPT-3 has a context window of 2048 tokens, and its estimated that the latest models of GPT-3.5 (the colloquial name for the underlying model used for creating ChatGPT is 4000 tokens).\n",
    "- Longer context windows allow the model to retain more information over long pieces of text, giving the impression of “memory”.\n",
    "- Refer to the paper for some clever mathematical sinusoidal “tricks” used to achieve this!\n",
    "- An embedding neural network is used to convert the tokens into a vector, GPT-3 initially used a 12,288 dimension vector.\n",
    "- The nature of the GPT style models do not lend themselves well to “on-edge” inference (i.e. running GPT-3 on your phone).\n",
    "\n",
    "**DALL-E**\n",
    "\n",
    "- In January of 2021, OpenAI announced work on DALL-E, and then one year later revealed DALL-E 2.\n",
    "- DALL-E 2 was initially released in private beta and then later opened up to the public, with an API built soon after.\n",
    "- The name comes from the combination of “WALL-E” and “Dali” (as in Salvador Dali).\n",
    "- Fundamentally, all DALL-E does is take in an input text string and output an image.\n",
    "- Note how overall this idea is actually quite similar to the idea of GPT-3, except in this case the modality of the output is different.\n",
    "- An embedding is the creation of a vector representation of an object, such as a text embedding, allowing us to represent a word as a vector of N-dimensions.\n",
    "\n",
    "##### There are actually two main stages:\n",
    "\n",
    "- Prior: Performs the text embedding, generating a CLIP image embedding.\n",
    "- Decoder (unCLIP): A diffusion model which actually generates the image from the prior embedding.\n",
    "\n",
    "##### Contrastive Language–Image Pre-training: CLIP is trained on image-text pairs\n",
    "\n",
    "1. Conrastive pre-training (Text Encoder-Image Encoder)\n",
    "2. Create dataset classifier from label text\n",
    "3. Use for zero-shot prediction\n",
    "\n",
    "#####\n",
    "\n",
    "- CLIP is only incentivized to learn the features of an image that are sufficient to match it up with the correct caption (as opposed to any of the others in the list).\n",
    "- This makes CLIP not ideal for learning about certain aspects of images, like relative positions of objects.\n",
    "- If you read the DALLE publication papers, you will notice the authors were curious to see what happened when attempting to just directly pass the text embedding directly to the encoder.\n",
    "- The authors noted much better results when using the additional Prior model using CLIP.\n",
    "- Intuitively, we can think of this as having one embedding for text meaning, and another embedding for the “gist” of an image from text.\n",
    "- An infinite number of images could be consistent with a given caption, so the outputs of the two encoders will not perfectly coincide.\n",
    "- Hence, a separate prior model is needed to “translate” the text embedding into an image embedding that could plausibly match it.\n",
    "\n",
    "##### Diffusion\n",
    "\n",
    "- A diffusion model is trained to undo the steps of a fixed corruption process.\n",
    "- Each step of the corruption process adds a small amount of gaussian noise to an image, which erases some of the information in it.\n",
    "- After the final step, the image becomes indistinguishable from pure noise.\n",
    "- The diffusion model is trained to reverse this process, and in doing so learns to regenerate what might have been erased in each step.\n",
    "\n",
    "##### Two Main Stages\n",
    "\n",
    "##### Prior Stage:\n",
    "\n",
    "- Generates the CLIP image embedding (intended to describe the “gist” of the image) from the given caption (which itself is actually a text embedding).\n",
    "\n",
    "##### Decoder Stage:\n",
    "\n",
    "- A diffusion model called unCLIP generates the image itself from this embedding.\n",
    "- unCLIP receives both a corrupted version of the image it is trained to reconstruct, as well as the CLIP image embedding of the clean image.\n",
    "\n",
    "#####\n",
    "\n",
    "- After these two stages an upsampling is performed on the image to get higher resolution.\n",
    "- DALLE-2 was trained on 512x512 images, so any higher resolution output is actually upscaled from 512x512.\n",
    "- An interesting aspect of image generation models is the multiple stages.\n",
    "- This means image generation models lend themselves to be run “on-edge”, in fact there have already been releases of Stable Diffusion models running locally on an iPhone!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
