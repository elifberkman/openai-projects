{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OPENAI INTRODUCTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Environment Variable for OpenAI Api Key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Read Api key from text file\n",
    "with open('openai-api-key.txt', 'r') as f:\n",
    "    openai_api_key = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create Environment Variable\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Give me two reasons to learn OpenAI API with Python',\n",
    "    max_tokens=300\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. OpenAI API with Python offers easy access to powerful open-source tools that can be used to develop powerful AI applications. This API offers a high level of flexibility, allowing developers to manipulate and control the underlying algorithms. \n",
      "\n",
      "2. OpenAI API with Python allows developers to create applications that are specifically tailored to the user's need, providing them with the power of advanced AI technology. This API also simplifies the process of creating complex AI applications, as it provides simple APIs and methods that allow developers to quickly and easily prototype AI applications.\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HISTORY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2015 - OpenAI as a Non-Profit**\n",
    "\n",
    "- In December 2015, at the end of the NIPS (Neural Information Processing Systems) conference in Montreal, Elon Musk and Sam Altman announced the creation of a new non-profit organization, called OpenAI\n",
    "\n",
    "**Sam Altman**\n",
    "\n",
    "- Dropped out of Stanford to create a location-based social network called Loopt in 2005, which was later acquired in 2012 for &#36;43.4 million.\n",
    "- In 2011 Altman started as a part-time partner at startup accelerator Y Combinator. In 2014, he became the president of YC.\n",
    "- Y Combinator super famous accelerator in Silicon Valley. Lots of famous companies have come from this, like Airbnb and Dropbox.\n",
    "- Even though OpenAI started in 2015, it wasn’t until 2019 that Altman transitioned away from being president of YC to focus on OpenAI.\n",
    "- Gary Tan is now currently the president of YC.\n",
    "\n",
    "**Elon Musk**\n",
    "\n",
    "<i>1995</i>\n",
    "- Starts Zip2 with his brother Kimbal Musk and Greg Kouri.\n",
    "- Sold to Compaq in 1999 for &#36; 307 million (Elon’s stake was worth about &#36; 7 million).\n",
    "\n",
    "<i>1999</i>\n",
    "- Starts online financial services company X.com, which in 2000 merged with Confinity to form PayPal.\n",
    "\n",
    "<i>2002</i>\n",
    "- Paypal is acquired by eBay for &#36;1.5 billion.\n",
    "- Musk’s stake was approximately &#36;176 million.\n",
    "- Musk founded the company SpaceX, a rocket manufacturer and launcher.\n",
    "- Later on SpaceX would develop its own satellite internet.\n",
    "\n",
    "<i>2004</i>\n",
    "- Musk invests in Tesla, an electric car company, becoming its largest shareholder.\n",
    "- Starting in 2005 Musk took a much more active role in Tesla.\n",
    "\n",
    "<i>2022</i>\n",
    "- Musk acquires Twitter for &#36;43 billion.\n",
    "\n",
    "<i>Other companies:</i>\n",
    "- Solar City (acquired by Tesla)\n",
    "- Boring Company\n",
    "- Neuralink\n",
    "- Starlink\n",
    "- OpenAI\n",
    "\n",
    "**Early Major Investors in OpenAI**\n",
    "\n",
    "- Reid Hoffman (founder of LinkedIn)\n",
    "- Jessica Livingston (one of the founders of YC)\n",
    "- Peter Thiel\n",
    "- Infosys\n",
    "- Khosla Ventures\n",
    "- YC Research\n",
    "\n",
    "**Early Employees at OpenAI**\n",
    "\n",
    "- Greg Brockman\n",
    "- Ilya Sutskever\n",
    "- Trevor Blackwell\n",
    "- Andrej Karpathy\n",
    "- Durk Kingma\n",
    "- Wojciech Zaremba\n",
    "- And many more well-regarded advisors!\n",
    "\n",
    "**OpenAI**\n",
    "\n",
    "- Initially formed as a non-profit to safely develop Artificial Intelligence.\n",
    "- Musk and Altman were influenced by Google acquiring DeepMind in 2014, concerned that A.I. technology would only be developed and controlled by just a few of the world’s largest technology companies.\n",
    "- In 2016, OpenAI released the Gym library, which allowed for an easy to use environment for reinforcement learning.\n",
    "- In 2018, OpenAI announced the first version of GPT (Generative Pre-Training Transformer).\n",
    "- In 2018 Elon Musk resigned his board seat at OpenAI, due to “potential future conflict of interest” due to Tesla’s own development of AI systems (mainly for self-driving cars at the time, but now Tesla is working on a humanoid robot called Optimus).\n",
    "- In 2019, OpenAI transitioned from a non-profit organization to a “capped” for-profit organization, in order to accept an investment of &#36; 1 billion dollars, partnering with Microsoft in the process (who was also the lead investor).\n",
    "- In 2019, OpenAI announces a new model called GPT-2.\n",
    "- GPT-2 was not initially released to the public due to safety concerns regarding the ability to possibly create false misinformation at a large scale.\n",
    "- In 2020, GPT-3 was announced.\n",
    "- In June of 2020 OpenAI would announced the creation of an API to access its new AI Models.\n",
    "- Initial users had to apply to be accepted to have access to the API.\n",
    "- In 2021, OpenAI announced the creation of DALL-E, a model capable of producing images from text.\n",
    "- The model is not open-sourced or available via an API.\n",
    "- In 2022, DALLE-2 is announced, creating much higher fidelity images from text prompts.\n",
    "- Also in 2022 ChatGPT is announced, which is an optimized version of GPT for dialogue, trained on human feedback.\n",
    "- At the start of 2023, OpenAI announced that Microsoft made a new &#36; 10 billion investment for OpenAI.\n",
    "- As part of this investment, Azure became the exclusive cloud provider of OpenAI model API calls. Note that the API is still available directly from OpenAI.\n",
    "- Clearly a lot has happened in just a short timespan, and the pace of development is exponential!\n",
    "- If you’re interested in learning more about this recent history of artificial intelligence and the role its played in the dynamics of technology companies, you may want to read Genius Makers by Metz."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOW IT WORKS?\n",
    "\n",
    "**GPT**\n",
    "\n",
    "- One of the more novel aspects of GPT-3 versus its predecessors GPT and GPT-2 was its size.\n",
    "- GPT-3 has 175 billion parameters, which in storage terms is approximately 800 GB.\n",
    "- It also cost about &#36;4.6 million dollars to train in GPU costs.\n",
    "- GPT-3 has a context window of 2048 tokens, and its estimated that the latest models of GPT-3.5 (the colloquial name for the underlying model used for creating ChatGPT is 4000 tokens).\n",
    "- Longer context windows allow the model to retain more information over long pieces of text, giving the impression of “memory”.\n",
    "- Refer to the paper for some clever mathematical sinusoidal “tricks” used to achieve this!\n",
    "- An embedding neural network is used to convert the tokens into a vector, GPT-3 initially used a 12,288 dimension vector.\n",
    "- The nature of the GPT style models do not lend themselves well to “on-edge” inference (i.e. running GPT-3 on your phone).\n",
    "\n",
    "**DALL-E**\n",
    "\n",
    "- In January of 2021, OpenAI announced work on DALL-E, and then one year later revealed DALL-E 2.\n",
    "- DALL-E 2 was initially released in private beta and then later opened up to the public, with an API built soon after.\n",
    "- The name comes from the combination of “WALL-E” and “Dali” (as in Salvador Dali).\n",
    "- Fundamentally, all DALL-E does is take in an input text string and output an image.\n",
    "- Note how overall this idea is actually quite similar to the idea of GPT-3, except in this case the modality of the output is different.\n",
    "- An embedding is the creation of a vector representation of an object, such as a text embedding, allowing us to represent a word as a vector of N-dimensions.\n",
    "\n",
    "##### There are actually two main stages:\n",
    "\n",
    "- Prior: Performs the text embedding, generating a CLIP image embedding.\n",
    "- Decoder (unCLIP): A diffusion model which actually generates the image from the prior embedding.\n",
    "\n",
    "##### Contrastive Language–Image Pre-training: CLIP is trained on image-text pairs\n",
    "\n",
    "1. Conrastive pre-training (Text Encoder-Image Encoder)\n",
    "2. Create dataset classifier from label text\n",
    "3. Use for zero-shot prediction\n",
    "\n",
    "#####\n",
    "\n",
    "- CLIP is only incentivized to learn the features of an image that are sufficient to match it up with the correct caption (as opposed to any of the others in the list).\n",
    "- This makes CLIP not ideal for learning about certain aspects of images, like relative positions of objects.\n",
    "- If you read the DALLE publication papers, you will notice the authors were curious to see what happened when attempting to just directly pass the text embedding directly to the encoder.\n",
    "- The authors noted much better results when using the additional Prior model using CLIP.\n",
    "- Intuitively, we can think of this as having one embedding for text meaning, and another embedding for the “gist” of an image from text.\n",
    "- An infinite number of images could be consistent with a given caption, so the outputs of the two encoders will not perfectly coincide.\n",
    "- Hence, a separate prior model is needed to “translate” the text embedding into an image embedding that could plausibly match it.\n",
    "\n",
    "##### Diffusion\n",
    "\n",
    "- A diffusion model is trained to undo the steps of a fixed corruption process.\n",
    "- Each step of the corruption process adds a small amount of gaussian noise to an image, which erases some of the information in it.\n",
    "- After the final step, the image becomes indistinguishable from pure noise.\n",
    "- The diffusion model is trained to reverse this process, and in doing so learns to regenerate what might have been erased in each step.\n",
    "\n",
    "##### Two Main Stages\n",
    "\n",
    "##### Prior Stage:\n",
    "\n",
    "- Generates the CLIP image embedding (intended to describe the “gist” of the image) from the given caption (which itself is actually a text embedding).\n",
    "\n",
    "##### Decoder Stage:\n",
    "\n",
    "- A diffusion model called unCLIP generates the image itself from this embedding.\n",
    "- unCLIP receives both a corrupted version of the image it is trained to reconstruct, as well as the CLIP image embedding of the clean image.\n",
    "\n",
    "#####\n",
    "\n",
    "- After these two stages an upsampling is performed on the image to get higher resolution.\n",
    "- DALLE-2 was trained on 512x512 images, so any higher resolution output is actually upscaled from 512x512.\n",
    "- An interesting aspect of image generation models is the multiple stages.\n",
    "- This means image generation models lend themselves to be run “on-edge”, in fact there have already been releases of Stable Diffusion models running locally on an iPhone!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AI SAFETY AND ALIGNMENT\n",
    "\n",
    "- Recall that the creation of OpenAI by Musk, Altman, and others was motivated by trying to make sure AI was developed in an open and safe manner.\n",
    "- Deeply embedded in the systems of OpenAI is attention to safety, alignment and biases.\n",
    "- If you’ve read the publications OpenAI has released about GPT-3 and DALLE-2, you would have noticed that a lot of work is done to prevent harm.\n",
    "- In your usage, you may notice this sometimes limits potential outputs.\n",
    "- You should also keep in mind that OpenAI is one of the most committed companies in this area of AI, especially in terms of research and development of AI safety and alignment.\n",
    "\n",
    "**limitations of text and image generation**\n",
    "\n",
    "- OpenAI will restrict outputs of prompts that violate their content policy.\n",
    "- These include topics like hate, threats, self-harm, sexual output, and violence.\n",
    "- For example, you can’t ask GPT-3 to teach you how to make an improvised explosive device, it will flag the request and filter it out.\n",
    "- You should also be aware that because the training data is based off the internet, there are inherent biases in the data!\n",
    "- The original GPT-3 paper “Language Models are Few-Shot Learners” does an excellent job of discussing fairness, bias, and representation.\n",
    "- OpenAI also publishes many papers focused just on AI safety and moderation.\n",
    "- DALLE-2 is also under the same moderation restrictions, you can not ask for images that would potentially violate the moderation policies.\n",
    "- This sometimes leans too much to “safety” for certain users preferences, for example:\n",
    " \"over the shoulder shot of a man\"\n",
    " \"over the shoulder view of a man\"\n",
    "- An interesting technical note is that since requests for GPT and DALLE are both text inputs, the same moderation check on input text can be done on either model.\n",
    "\n",
    "**practice text prompts safely**\n",
    "\n",
    "- However, if you are concerned about potentially triggering the content policy limitations (which is violated too many times may pause your access), there is a moderation endpoint where you can check queries before actually getting results.\n",
    "- More information and details can be found here: [platform.openai.com/docs/guides/moderation/](platform.openai.com/docs/guides/moderation/)\n",
    "- While it may feel annoying at first for the models to be very inclined to “be safe”, keep in mind that this is one of the areas that OpenAI is actually most “open”, and you can read their publications to understand their thought processes and methodologies.\n",
    "- For full information on OpenAI’s policies, rules, and limitations, check out their platform policy page:[platform.openai.com/docs/usage-policies ](platform.openai.com/docs/usage-policies )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"categories\": {\n",
      "    \"hate\": false,\n",
      "    \"hate/threatening\": false,\n",
      "    \"self-harm\": false,\n",
      "    \"sexual\": false,\n",
      "    \"sexual/minors\": false,\n",
      "    \"violence\": false,\n",
      "    \"violence/graphic\": false\n",
      "  },\n",
      "  \"category_scores\": {\n",
      "    \"hate\": 4.921302206639666e-06,\n",
      "    \"hate/threatening\": 1.0990176546599173e-09,\n",
      "    \"self-harm\": 8.864341261016762e-09,\n",
      "    \"sexual\": 2.6443567548994906e-05,\n",
      "    \"sexual/minors\": 2.4819328814373876e-07,\n",
      "    \"violence\": 2.1955165721010417e-05,\n",
      "    \"violence/graphic\": 5.248724392004078e-06\n",
      "  },\n",
      "  \"flagged\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Moderation endpoint\n",
    "reponse_mod = openai.Moderation.create(\n",
    "    input=\"Sample text goes here\"\n",
    ")\n",
    "print(reponse_mod[\"results\"][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
